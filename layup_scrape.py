# -*- coding: utf-8 -*-
"""layup_scrape.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QNhZJ2OwFnq3_2KzyD7ngXin1dL6vuKH
"""

import requests
from bs4 import BeautifulSoup

# URL of the website to scrape
url = "https://www.layuplist.com/search?q=AAAS"


response = requests.get(url)

# Parse the HTML content of the page
soup = BeautifulSoup(response.text, "html.parser")

table = soup.find("table", {"class": "table"})


headers = [header.text.strip() for header in table.find_all("th")]

# Initialize a list to store the data rows
data = []

for row in table.find_all("tr")[1:]:

    row_data = [cell.text.strip() for cell in row.find_all("td")]

    data.append(row_data)

print("Headers:", headers)
for row in data:
    print(row)

import csv
import requests
from bs4 import BeautifulSoup

# URL of the website to scrape
url = "https://www.layuplist.com/search?q=AAAS"


response = requests.get(url)

soup = BeautifulSoup(response.text, "html.parser")


table = soup.find("table", {"class": "table"})


headers = [header.text.strip() for header in table.find_all("th")]

data = []

for row in table.find_all("tr")[1:]:
    # Extract the text from each cell in the row
    row_data = [cell.text.strip() for cell in row.find_all("td")]

    data.append(row_data)


csv_file = "layuplist_aaas.csv"


with open(csv_file, "w", newline="", encoding="utf-8") as file:

    writer = csv.writer(file)
    # Write the headers to the CSV file
    writer.writerow(headers)

    writer.writerows(data)

print(f"Data has been scraped and saved to {csv_file}")

import csv
import requests
from bs4 import BeautifulSoup

def scrape_and_save_course_data(course_code, csv_suffix):
    # Construct the URL for the course code
    url = f"https://www.layuplist.com/search?q={course_code}"

    # Send a GET request to the URL
    response = requests.get(url)

    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, "html.parser")

    # Find the table containing the course information
    table = soup.find("table", {"class": "table"})

    # Extract the headers (column names) from the table
    headers = [header.text.strip() for header in table.find_all("th")]

    # Initialize a list to store the data rows
    data = []

    # Extract the rows of data from the table
    for row in table.find_all("tr")[1:]:
        # Extract the text from each cell in the row
        row_data = [cell.text.strip() for cell in row.find_all("td")]
        # Append the row data to the list
        data.append(row_data)

    # Define the name of the CSV file
    csv_file = f"layuplist_{csv_suffix}.csv"

    # Write the data to a CSV file
    with open(csv_file, "w", newline="", encoding="utf-8") as file:
        # Create a CSV writer object
        writer = csv.writer(file)
        # Write the headers to the CSV file
        writer.writerow(headers)
        # Write the data rows to the CSV file
        writer.writerows(data)

    print(f"Data for {course_code} has been scraped and saved to {csv_file}")

# List of course codes
course_codes = [
    "AAAS", "AMEL", "AMES", "ANTH", "ARAB", "ARTH", "ASCL", "ASTR", "BIOL", "CHEM",
    "CHIN", "CLST", "COCO", "COGS", "COLT", "COSC", "CRWT", "EARS", "ECON", "EDUC",
    "ENGL", "ENGS", "ENVS", "FILM", "FREN", "FRIT", "GEOG", "GERM", "GOVT", "GRK",
    "HCDS", "HEBR", "HIST", "HUM", "INTS", "ITAL", "JAPN", "JWST", "LACS", "LAT",
    "LATS", "LING", "MATH", "MES", "MUS", "NAIS", "NAS", "PBPL", "PHIL", "PHYS",
    "PORT", "PSYC", "QSS", "REL", "RUSS", "SART", "SOCY", "SPAN", "SPEE", "SSOC",
    "THEA", "TUCK", "WGSS", "WRIT"
]

# Iterate over the course codes and scrape data for each
for index, course_code in enumerate(course_codes, start=1):
    scrape_and_save_course_data(course_code, index)

import csv
import os
import requests
from bs4 import BeautifulSoup

def scrape_and_save_course_data(course_code, csv_suffix):
    # Construct the URL for the course code
    url = f"https://www.layuplist.com/search?q={course_code}"

    response = requests.get(url)

    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, "html.parser")

    # Find sthe table containing the course information
    table = soup.find("table", {"class": "table"})

    headers = [header.text.strip() for header in table.find_all("th")]

    # Initializes a list to store the data rows
    data = []

    # Extract the rows of data from the table
    for row in table.find_all("tr")[1:]:
        # Extracts the text from each cell in the row
        row_data = [cell.text.strip() for cell in row.find_all("td")]
        # Append the row data to the list
        data.append(row_data)

    # Define the name of the CSV file
    csv_file = f"layuplist_{csv_suffix}.csv"

    #CSV file
    with open(csv_file, "w", newline="", encoding="utf-8") as file:
        # Create a CSV writer object
        writer = csv.writer(file)
        # Write the headers to the CSV file
        writer.writerow(headers)
        # Write the data rows to the CSV file
        writer.writerows(data)

    print(f"Data for {course_code} has been scraped and saved to {csv_file}")

# List of course codes
course_codes = [
    "AAAS", "AMEL", "AMES", "ANTH", "ARAB", "ARTH", "ASCL", "ASTR", "BIOL", "CHEM",
    "CHIN", "CLST", "COCO", "COGS", "COLT", "COSC", "CRWT", "EARS", "ECON", "EDUC",
    "ENGL", "ENGS", "ENVS", "FILM", "FREN", "FRIT", "GEOG", "GERM", "GOVT", "GRK",
    "HCDS", "HEBR", "HIST", "HUM", "INTS", "ITAL", "JAPN", "JWST", "LACS", "LAT",
    "LATS", "LING", "MATH", "MES", "MUS", "NAIS", "NAS", "PBPL", "PHIL", "PHYS",
    "PORT", "PSYC", "QSS", "REL", "RUSS", "SART", "SOCY", "SPAN", "SPEE", "SSOC",
    "THEA", "TUCK", "WGSS", "WRIT"
]

# Name of the final combined CSV file
final_csv_file = "Final_list.csv"

# Check if the final CSV file already exists and delete it if it does
if os.path.exists(final_csv_file):
    os.remove(final_csv_file)

# Write the data to the final CSV file
with open(final_csv_file, "a", newline="", encoding="utf-8") as final_file:
    final_writer = csv.writer(final_file)

    # Write the header row to the final CSV file
    final_writer.writerow(["Code", "Department Name", "Undergrad Courses", "Quality", "Difficulty", "Workload"])

    # Iterate over each individual CSV file
    for index, course_code in enumerate(course_codes, start=1):
        scrape_and_save_course_data(course_code, index)

       #CSV file
        csv_file = f"layuplist_{index}.csv"

        # Check if the current CSV file exists
        if os.path.exists(csv_file):
            # Read the data from the current CSV file and append it to the final CSV file
            with open(csv_file, "r", newline="", encoding="utf-8") as file:
                reader = csv.reader(file)
                next(reader)  # Skip the header row
                for row in reader:
                    final_writer.writerow([course_code] + row)

print(f"Combined into {final_csv_file}")